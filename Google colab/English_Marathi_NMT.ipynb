{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_Marathi_NMT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl78EwQfBg6y"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import *"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hr_0UYaCDs_"
      },
      "source": [
        "num_samples = 10000\n",
        "data_path = r\"/content/drive/MyDrive/mar.txt\"\n",
        "\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    # \\n - new line\n",
        "    # List containing each line in file as a value\n",
        "    lines = f.read().split(\"\\n\")\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "# len(lines) = 42703\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "\n",
        "    # \"\\t\" - tabs\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "\n",
        "    # Adding start & end tokens\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "\n",
        "    # Creating character vocabulary\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWUZsgfcCDvv"
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "# No.of samples: len(input_texts)\n",
        "# No.of unique input tokens: num_encoder_tokens\n",
        "# No.of unique output tokens: num_decoder_tokens\n",
        "# Maximum seq length for inputs: max_encoder_seq_length\n",
        "# Maximum seq length for outputs: max_decoder_seq_length"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7b44hu5CDyl"
      },
      "source": [
        "# creating character-to-index lookup dictionary\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tpqlVebCD3d"
      },
      "source": [
        "# Create 3D arrays to store all one-hot encoded sentences.\n",
        "# This 3D array contains \"n\" 2D arrays where n=no.of samples.Each sentence is represented as 2D array\n",
        "# Each 2D array contains \"n\" 1D arrays where n=max_seq_length.Each word is represented by 1D array\n",
        "# Each 1D array contains \"n\" elements where n=no.of unique characters in data.Each char in word is represented by 1/0\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXO74UquCD6X"
      },
      "source": [
        "# Building the encoder & decoder model for Training Phase.\n",
        "\n",
        "latent_dim = 256\n",
        "\n",
        "# encoder\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True) #lstm_1\n",
        "\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "#decoder\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STgjZPb2CD8e",
        "outputId": "e5264c08-db62-4ac3-b3c3-5b87c63150b0"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "# we have 2 inputs (1 for encoder & 1 for decoder since we are following \"teacher-forcing\")\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.001),\n",
        "              loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# Custom Keras callback to stop training when certain accuracy is achieved.\n",
        "from keras.callbacks import Callback\n",
        "class MyThresholdCallback(Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(MyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_acc = logs[\"val_accuracy\"]\n",
        "        if val_acc >= self.threshold:\n",
        "            self.model.stop_training = True\n",
        "            model_name = (\"IMDB_sentiment_analysis_\" + str(val_acc))\n",
        "            model.save(model_name)\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],decoder_target_data,\n",
        "          batch_size=batch_size,epochs=epochs,validation_split=0.2,callbacks=[MyThresholdCallback(threshold=0.9)])\n",
        "\n",
        "# Saving the Model\n",
        "model.save(\"Eng_Marathi_NMT\")\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "125/125 [==============================] - 42s 313ms/step - loss: 1.3978 - accuracy: 0.6763 - val_loss: 1.1146 - val_accuracy: 0.7008\n",
            "Epoch 2/30\n",
            "125/125 [==============================] - 38s 306ms/step - loss: 0.8053 - accuracy: 0.7788 - val_loss: 0.9390 - val_accuracy: 0.7389\n",
            "Epoch 3/30\n",
            "125/125 [==============================] - 38s 307ms/step - loss: 0.6942 - accuracy: 0.8046 - val_loss: 0.8540 - val_accuracy: 0.7582\n",
            "Epoch 4/30\n",
            "125/125 [==============================] - 38s 308ms/step - loss: 0.6304 - accuracy: 0.8209 - val_loss: 0.7919 - val_accuracy: 0.7779\n",
            "Epoch 5/30\n",
            "125/125 [==============================] - 38s 307ms/step - loss: 0.5791 - accuracy: 0.8342 - val_loss: 0.7573 - val_accuracy: 0.7868\n",
            "Epoch 6/30\n",
            "125/125 [==============================] - 39s 309ms/step - loss: 0.5422 - accuracy: 0.8439 - val_loss: 0.7368 - val_accuracy: 0.7920\n",
            "Epoch 7/30\n",
            "125/125 [==============================] - 39s 310ms/step - loss: 0.5152 - accuracy: 0.8495 - val_loss: 0.7243 - val_accuracy: 0.7943\n",
            "Epoch 8/30\n",
            "125/125 [==============================] - 39s 310ms/step - loss: 0.4821 - accuracy: 0.8603 - val_loss: 0.7060 - val_accuracy: 0.8065\n",
            "Epoch 9/30\n",
            "125/125 [==============================] - 39s 308ms/step - loss: 0.4536 - accuracy: 0.8678 - val_loss: 0.6865 - val_accuracy: 0.8114\n",
            "Epoch 10/30\n",
            "125/125 [==============================] - 39s 308ms/step - loss: 0.4301 - accuracy: 0.8740 - val_loss: 0.6816 - val_accuracy: 0.8147\n",
            "Epoch 11/30\n",
            "125/125 [==============================] - 39s 310ms/step - loss: 0.4091 - accuracy: 0.8802 - val_loss: 0.6751 - val_accuracy: 0.8181\n",
            "Epoch 12/30\n",
            "125/125 [==============================] - 39s 310ms/step - loss: 0.3910 - accuracy: 0.8850 - val_loss: 0.6698 - val_accuracy: 0.8202\n",
            "Epoch 13/30\n",
            "125/125 [==============================] - 39s 309ms/step - loss: 0.3742 - accuracy: 0.8893 - val_loss: 0.6790 - val_accuracy: 0.8188\n",
            "Epoch 14/30\n",
            "125/125 [==============================] - 38s 308ms/step - loss: 0.3705 - accuracy: 0.8901 - val_loss: 0.6733 - val_accuracy: 0.8198\n",
            "Epoch 15/30\n",
            "125/125 [==============================] - 39s 309ms/step - loss: 0.3531 - accuracy: 0.8948 - val_loss: 0.6793 - val_accuracy: 0.8206\n",
            "Epoch 16/30\n",
            "125/125 [==============================] - 39s 310ms/step - loss: 0.3379 - accuracy: 0.8990 - val_loss: 0.6842 - val_accuracy: 0.8211\n",
            "Epoch 17/30\n",
            "125/125 [==============================] - 39s 309ms/step - loss: 0.3807 - accuracy: 0.8860 - val_loss: 0.6764 - val_accuracy: 0.8211\n",
            "Epoch 18/30\n",
            "125/125 [==============================] - 39s 310ms/step - loss: 0.3397 - accuracy: 0.8982 - val_loss: 0.6799 - val_accuracy: 0.8220\n",
            "Epoch 19/30\n",
            "125/125 [==============================] - 39s 308ms/step - loss: 0.3176 - accuracy: 0.9048 - val_loss: 0.6805 - val_accuracy: 0.8252\n",
            "Epoch 20/30\n",
            "125/125 [==============================] - 41s 326ms/step - loss: 0.3066 - accuracy: 0.9075 - val_loss: 0.6982 - val_accuracy: 0.8227\n",
            "Epoch 21/30\n",
            "125/125 [==============================] - 39s 309ms/step - loss: 0.2933 - accuracy: 0.9115 - val_loss: 0.6915 - val_accuracy: 0.8253\n",
            "Epoch 22/30\n",
            "125/125 [==============================] - 39s 308ms/step - loss: 0.2808 - accuracy: 0.9150 - val_loss: 0.6980 - val_accuracy: 0.8254\n",
            "Epoch 23/30\n",
            "125/125 [==============================] - 39s 308ms/step - loss: 0.2697 - accuracy: 0.9182 - val_loss: 0.7113 - val_accuracy: 0.8252\n",
            "Epoch 24/30\n",
            "125/125 [==============================] - 39s 311ms/step - loss: 0.2602 - accuracy: 0.9213 - val_loss: 0.7121 - val_accuracy: 0.8253\n",
            "Epoch 25/30\n",
            "125/125 [==============================] - 39s 309ms/step - loss: 0.2512 - accuracy: 0.9236 - val_loss: 0.7234 - val_accuracy: 0.8250\n",
            "Epoch 26/30\n",
            "125/125 [==============================] - 39s 310ms/step - loss: 0.2430 - accuracy: 0.9262 - val_loss: 0.7224 - val_accuracy: 0.8243\n",
            "Epoch 27/30\n",
            "125/125 [==============================] - 39s 309ms/step - loss: 0.2349 - accuracy: 0.9287 - val_loss: 0.7351 - val_accuracy: 0.8257\n",
            "Epoch 28/30\n",
            "125/125 [==============================] - 39s 308ms/step - loss: 0.2272 - accuracy: 0.9306 - val_loss: 0.7399 - val_accuracy: 0.8254\n",
            "Epoch 29/30\n",
            "125/125 [==============================] - 38s 307ms/step - loss: 0.2205 - accuracy: 0.9329 - val_loss: 0.7559 - val_accuracy: 0.8252\n",
            "Epoch 30/30\n",
            "125/125 [==============================] - 38s 307ms/step - loss: 0.2133 - accuracy: 0.9351 - val_loss: 0.7516 - val_accuracy: 0.8260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_16_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn, lstm_cell_17_layer_call_and_return_conditional_losses, lstm_cell_17_layer_call_fn, lstm_cell_16_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Eng_Marathi_NMT/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Eng_Marathi_NMT/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-xMO8yXCEBt"
      },
      "source": [
        "# Building the encoder & decoder model again for Testing Phase since we are using\n",
        "# \"Teacher-Forcing\" decoder works differently during training & testing.\n",
        "\n",
        "# encoder\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# decoder\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4_\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81FCvNOeCEEV"
      },
      "source": [
        "# Reverse-lookup (index-to-character) to decode the sequence to make it readable\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abBd2awdCEHJ"
      },
      "source": [
        "# After generating the empty sequence of length 1, the model should know when\n",
        "# to start and stop reading the text.To read the model will check out for \\t in this case.\n",
        "# Keep two conditions, either when the max length of sentence is hit or find stop character \\n.\n",
        "# Keep on updating the target sequence by one and update the states.\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaZad69YC9sL",
        "outputId": "3cc3c09b-9a4e-48a5-a80d-860531f8d5f6"
      },
      "source": [
        "# Picks a random sentence from dataset & translates it\n",
        "\n",
        "i = np.random.choice(len(input_texts))\n",
        "input_seq = encoder_input_data[i:i+1]\n",
        "translation = decode_sequence(input_seq)\n",
        "\n",
        "print('English :', input_texts[i])\n",
        "print('Marathi Translation:', translation)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English : He has a blog.\n",
            "Marathi Translation: त्यांच्याकडे एक गाडी आहे.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}